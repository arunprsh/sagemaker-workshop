{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird Classification with PyTorch on Amazon SageMaker - Directly in your notebook\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Data Preparation](#Data-Preparation)\n",
    "3. [Train the model](#Train-the-model)\n",
    "4. [Test the model](#Test-the-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Image classification is an increasingly popular machine learning technique, in which a trained model predicts which of several classes is represented by a particular image. This technique is useful across a wide variety of use cases from manufacturing quality control to medical diagnosis. To create an image classification solution, we need to acquire and process an image dataset, and train a model from that dataset. The trained model is then capable of identifying features and predicting which class an image belongs to. Finally, we can make predictions using the trained model against previously unseen images.\n",
    "\n",
    "This notebook is an end-to-end example showing how to build an image classifier using PyTorch from Amazon SageMaker's hosted Jupyter notebook directly. This is an easy transition from traditional machine learning development you may already be doing on your laptop or on an Amazon EC2 instance. Subsequent notebooks in this workshop demonstrate how to take full advantage of SageMaker's training service, hosting service, batch inference, and automatic model tuning. \n",
    "\n",
    "For each of the labs in this workshop, we use a publicly available set of bird images based on the [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset. We demonstrate transfer learning by leveraging pretrained ImageNet weights for a ResNet50 network architecture.\n",
    "\n",
    "For a quick demonstration, pick a small handful of bird species (set `SAMPLE_ONLY = True` and choose a few classes / species). For a more complete model, you can train against all 200 bird species in the dataset. For anything more than a few classes, be sure to upgrade your notebook instance type to one of SageMaker's GPU instance types (ml.p2, ml.p3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The [Caltech Birds (CUB 200 2011)](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html) dataset contains 11,788 images across 200 bird species (the original technical report can be found [here](http://www.vision.caltech.edu/visipedia/papers/CUB_200_2011.pdf)).  Each species comes with around 60 images, with a typical size of about 350 pixels by 500 pixels.  Bounding boxes are provided, as are annotations of bird parts.  A recommended train/test split is given, but image size data is not.\n",
    "\n",
    "![](./cub_200_2011_snapshot.png)\n",
    "\n",
    "The dataset can be downloaded [here](http://www.vision.caltech.edu/visipedia/CUB-200-2011.html).\n",
    "\n",
    "### Download and unpack the dataset\n",
    "\n",
    "Here we download the birds dataset from CalTech. You can do this once and keep the unpacked dataset in your notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import urllib.request\n",
    "\n",
    "def download(url):\n",
    "    filename = url.split('/')[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#download('http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Clean up prior version of the downloaded dataset if you are running this again\n",
    "#!rm -rf CUB_200_2011  \n",
    "\n",
    "# Unpack and then remove the downloaded compressed tar file\n",
    "#!gunzip -c ./CUB_200_2011.tgz | tar xopf - \n",
    "#!rm CUB_200_2011.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some parameters for the rest of the notebook to use\n",
    "Here we define a few parameters that help drive the rest of the notebook.  For example, `SAMPLE_ONLY` is defaulted to `True`. This will force the notebook to train on only a handful of species.  Setting `SAMPLE_ONLY` to false will make the notebook work with the entire dataset of 200 bird species.  This makes the training a more difficult challenge, and you will need to tune parameters and run more epochs.\n",
    "\n",
    "An `EXCLUDE_IMAGE_LIST` is defined as a mechanism to address any corrupt images from the dataset and ensure they do not disrupt the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# To speed up training and experimenting, you can use a small handful of species.\n",
    "# To see the full list of the classes available, look at the content of CLASSES_FILE.\n",
    "SAMPLE_ONLY  = True\n",
    "CLASSES = [13, 17, 35, 36, 47, 68, 73, 87]\n",
    "CLASSES = [13, 17, 35, 47]\n",
    "\n",
    "# Otherwise, you can use the full set of species\n",
    "if (not SAMPLE_ONLY):\n",
    "    CLASSES = []\n",
    "    for c in range(200):\n",
    "        CLASSES += [c + 1]\n",
    "\n",
    "BASE_DIR   = 'CUB_MINI/' #'CUB_200_2011/'\n",
    "IMAGES_DIR = BASE_DIR + 'images/'\n",
    "\n",
    "CLASSES_FILE = BASE_DIR + 'classes.txt'\n",
    "IMAGE_FILE   = BASE_DIR + 'images.txt'\n",
    "LABEL_FILE   = BASE_DIR + 'image_class_labels.txt'\n",
    "\n",
    "SPLIT_RATIOS = (0.7, 0.2, 0.1)\n",
    "\n",
    "CLASS_COLS      = ['class_number','class_id']\n",
    "\n",
    "EXCLUDE_IMAGE_LIST = ['087.Mallard/Mallard_0130_76836.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the dataset\n",
    "Show the list of bird species or dataset classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_df = pd.read_csv(CLASSES_FILE, sep=' ', names=CLASS_COLS, header=None)\n",
    "criteria = classes_df['class_number'].isin(CLASSES)\n",
    "classes_df = classes_df[criteria]\n",
    "\n",
    "class_name_list = sorted(classes_df['class_id'].unique().tolist())\n",
    "print(class_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each species, there are dozens of images of various shapes and sizes. By dividing the entire dataset into individual named (numbered) folders, the images are in effect labelled for supervised learning using image classification and object detection algorithms. \n",
    "\n",
    "The following function displays a grid of thumbnail images for all the image files for a given species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_species(species_id):\n",
    "    _im_list = !ls $IMAGES_DIR/$species_id\n",
    "\n",
    "    NUM_COLS = 4\n",
    "    IM_COUNT = len(_im_list)\n",
    "\n",
    "    print('Species ' + species_id + ' has ' + str(IM_COUNT) + ' images.')\n",
    "    \n",
    "    NUM_ROWS = int(IM_COUNT / NUM_COLS)\n",
    "    if ((IM_COUNT % NUM_COLS) > 0):\n",
    "        NUM_ROWS += 1\n",
    "\n",
    "    fig, axarr = plt.subplots(NUM_ROWS, NUM_COLS)\n",
    "    fig.set_size_inches(12.0, 20.0, forward=True)\n",
    "\n",
    "    curr_row = 0\n",
    "    for curr_img in range(IM_COUNT):\n",
    "        # fetch the url as a file type object, then read the image\n",
    "        f = IMAGES_DIR + species_id + '/' + _im_list[curr_img]\n",
    "        a = plt.imread(f)\n",
    "\n",
    "        # find the column by taking the current index modulo 3\n",
    "        col = curr_img % NUM_ROWS\n",
    "        # plot on relevant subplot\n",
    "        axarr[col, curr_row].imshow(a)\n",
    "        if col == (NUM_ROWS - 1):\n",
    "            # we have finished the current row, so increment row counter\n",
    "            curr_row += 1\n",
    "\n",
    "    fig.tight_layout()       \n",
    "    plt.show()\n",
    "        \n",
    "    # Clean up\n",
    "    plt.clf()\n",
    "    plt.cla()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_species('013.Bobolink')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train/val/test dataframes from our dataset\n",
    "Here we split our dataset into training, testing, and validation datasets, each in their own Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_train_val_test(df, label_column, splits=(0.7, 0.2, 0.1), verbose=False):\n",
    "    train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    labels = df[label_column].unique()\n",
    "    for lbl in labels:\n",
    "        lbl_df = df[df[label_column] == lbl]\n",
    "\n",
    "        lbl_train_df        = lbl_df.sample(frac=splits[0])\n",
    "        lbl_val_and_test_df = lbl_df.drop(lbl_train_df.index)\n",
    "        lbl_test_df         = lbl_val_and_test_df.sample(frac=splits[2]/(splits[1] + splits[2]))\n",
    "        lbl_val_df          = lbl_val_and_test_df.drop(lbl_test_df.index)\n",
    "\n",
    "        if verbose:\n",
    "            print('\\n{}:\\n---------\\ntotal:{}\\ntrain_df:{}\\nval_df:{}\\ntest_df:{}'.format(lbl,\n",
    "                                                                        len(lbl_df), \n",
    "                                                                        len(lbl_train_df), \n",
    "                                                                        len(lbl_val_df), \n",
    "                                                                        len(lbl_test_df)))\n",
    "        train_df = train_df.append(lbl_train_df)\n",
    "        val_df   = val_df.append(lbl_val_df)\n",
    "        test_df  = test_df.append(lbl_test_df)\n",
    "\n",
    "    # shuffle them on the way out using .sample(frac=1)\n",
    "    return train_df.sample(frac=1), val_df.sample(frac=1), test_df.sample(frac=1)\n",
    "\n",
    "def get_train_val_dataframes():\n",
    "    images_df = pd.read_csv(IMAGE_FILE, sep=' ',\n",
    "                            names=['image_pretty_name', 'image_file_name'],\n",
    "                            header=None)\n",
    "    image_class_labels_df = pd.read_csv(LABEL_FILE, sep=' ',\n",
    "                                names=['image_pretty_name', 'orig_class_id'], header=None)\n",
    "\n",
    "    # Merge the metadata into a single flat dataframe for easier processing\n",
    "    full_df = pd.DataFrame(images_df)\n",
    "    full_df = full_df[~full_df.image_file_name.isin(EXCLUDE_IMAGE_LIST)]\n",
    "\n",
    "    full_df.reset_index(inplace=True, drop=True)\n",
    "    full_df = pd.merge(full_df, image_class_labels_df, on='image_pretty_name')\n",
    "\n",
    "    if SAMPLE_ONLY:\n",
    "        # grab a small subset of species for testing\n",
    "        criteria = full_df['orig_class_id'].isin(CLASSES)\n",
    "        full_df = full_df[criteria]\n",
    "        print('Using subset of total images based on sample class list. subtotal: {}'.format(full_df.shape[0]))\n",
    "\n",
    "    unique_classes = full_df['orig_class_id'].drop_duplicates()\n",
    "    sorted_unique_classes = sorted(unique_classes)\n",
    "    id_to_one_based = {}\n",
    "    i = 1\n",
    "    for c in sorted_unique_classes:\n",
    "        id_to_one_based[c] = str(i)\n",
    "        i += 1\n",
    "\n",
    "    full_df['class_id'] = full_df['orig_class_id'].map(id_to_one_based)\n",
    "    full_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    def get_class_name(fn):\n",
    "        return fn.split('/')[0]\n",
    "    full_df['class_name'] = full_df['image_file_name'].apply(get_class_name)\n",
    "    full_df = full_df.drop(['image_pretty_name'], axis=1)\n",
    "\n",
    "    train_df = []\n",
    "    test_df  = []\n",
    "    val_df   = []\n",
    "\n",
    "    # split into training and validation sets\n",
    "    train_df, val_df, test_df = split_to_train_val_test(full_df, 'class_id', SPLIT_RATIOS)\n",
    "\n",
    "    train_df.reset_index(inplace=True, drop=True)\n",
    "    val_df.reset_index(inplace=True, drop=True)\n",
    "    test_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    print('num images total: ' + str(images_df.shape[0]))\n",
    "    print('\\nnum train: ' + str(train_df.shape[0]))\n",
    "    print('num val: ' + str(val_df.shape[0]))\n",
    "    print('num test: ' + str(test_df.shape[0]))\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = get_train_val_dataframes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model \n",
    "Here we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 224\n",
    "WIDTH  = 224\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'PyTorch version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare image data generators from our dataframes\n",
    "Instead of having to make copies of all the images into separate train, test, and validation folders, we would like to leave the images in place. To let PyTorch train and test against these datasets, we create a custom PyTorch dataset that returns images and labels based on the images idenitified in a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, transform=None, dataframe=None):\n",
    "        self.data = dataframe\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        fn = IMAGES_DIR + self.data.loc[index]['image_file_name']\n",
    "        \n",
    "        # make the label 0-based, not 1-based\n",
    "        label = int(self.data.loc[index]['class_id']) - 1\n",
    "        \n",
    "        image = Image.open(fn)\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = torch.tensor(label)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(#file_path=None,\n",
    "                             transform=transforms.Compose([\n",
    "                                   transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "                                   transforms.RandomRotation(degrees=15),\n",
    "                                   transforms.RandomHorizontalFlip(),\n",
    "                                   transforms.CenterCrop(size=224),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                        std= [0.229, 0.224, 0.225])]), \n",
    "                             dataframe=train_df)\n",
    "val_dataset = ImageDataset(#file_path=None,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(size=256),\n",
    "                               transforms.CenterCrop(size=224),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                    std= [0.229, 0.224, 0.225])]), \n",
    "                           dataframe=val_df)\n",
    "test_dataset = ImageDataset(#file_path=None,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(size=256),\n",
    "                               transforms.CenterCrop(size=224),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                    std= [0.229, 0.224, 0.225])]), \n",
    "                           dataframe=test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "test_sampler = SubsetRandomSampler(test_df.index.values.tolist())\n",
    "testloader   = torch.utils.data.DataLoader(test_dataset,\n",
    "                   sampler=test_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_df.index.values.tolist())\n",
    "trainloader   = torch.utils.data.DataLoader(train_dataset,\n",
    "                   sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "val_sampler = SubsetRandomSampler(val_df.index.values.tolist())\n",
    "valloader   = torch.utils.data.DataLoader(val_dataset,\n",
    "                   sampler=val_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Will process {}/{} ({:.0f}%) of train data'.format(\n",
    "        len(trainloader.sampler), len(trainloader.dataset),\n",
    "        100. * len(trainloader.sampler) / len(trainloader.dataset)))\n",
    "print('Will process {}/{} ({:.0f}%) of train data'.format(\n",
    "        len(valloader.sampler), len(valloader.dataset),\n",
    "        100. * len(valloader.sampler) / len(valloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trainloader))\n",
    "for batch_idx, (data, target) in enumerate(trainloader, 1):\n",
    "    print(f'Batch #{batch_idx}, target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() \n",
    "                                  else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True, progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "fc_inputs = model.fc.in_features\n",
    "model.fc = nn.Sequential(nn.Linear(fc_inputs, 256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.3),\n",
    "                                 nn.Linear(256, len(class_name_list)),\n",
    "                                 nn.LogSoftmax(dim=1)) # for using NLLLoss()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.003)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {count_parameters(model):,d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform training and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 5\n",
    "steps  = 0\n",
    "running_loss = 0\n",
    "print_every  = 3\n",
    "train_losses, test_losses, test_accuracies = [], [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logps = model.forward(inputs)\n",
    "        loss  = criterion(logps, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in valloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "                    test_loss += batch_loss.item()\n",
    "                    \n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "            test_accuracies.append(accuracy/len(valloader))\n",
    "            train_losses.append(running_loss/len(trainloader))\n",
    "            test_losses.append(test_loss/len(valloader))                    \n",
    "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                  f\"Test loss: {test_loss/len(valloader):.3f}.. \"\n",
    "                  f\"Test accuracy: {accuracy/len(valloader):.3f}\")\n",
    "            running_loss = 0\n",
    "            model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy and loss across epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_accuracies, label='Test accuracy')\n",
    "plt.legend(frameon=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([\n",
    "                               transforms.Resize(size=256),\n",
    "                               transforms.CenterCrop(size=224),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                    std= [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=torch.load('model.pth', map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model     = models.resnet50(pretrained=True)\n",
    "fc_inputs = model.fc.in_features\n",
    "model.fc  = nn.Sequential(nn.Linear(fc_inputs, 256),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(0.3),\n",
    "                                 nn.Linear(256, len(class_name_list)),\n",
    "                                 nn.LogSoftmax(dim=1)) # for using NLLLoss()\n",
    "model.load_state_dict(torch.load('model.pth', map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def predict_image(image):\n",
    "    image_tensor = test_transforms(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    ps = torch.exp(output).detach().cpu().numpy()\n",
    "    index = output.data.cpu().numpy().argmax()\n",
    "    conf = ps[0][index]\n",
    "    return index, conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_images(df, num):\n",
    "    sample_df = df.sample(num)\n",
    "    sample_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(num):\n",
    "        fn = IMAGES_DIR + sample_df.loc[i]['image_file_name']\n",
    "        # make the label 0-based, not 1-based\n",
    "        lbl = int(sample_df.loc[i]['class_id']) - 1\n",
    "        img = Image.open(fn)\n",
    "        images.append(img)\n",
    "        labels.append(lbl)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = get_random_images(train_df, 7)\n",
    "fig=plt.figure(figsize=(20,20))\n",
    "classes = class_name_list \n",
    "for ii in range(len(images)):\n",
    "    image = images[ii]\n",
    "    index, conf = predict_image(image)\n",
    "    sub = fig.add_subplot(1, len(images), ii+1)\n",
    "    res = int(labels[ii]) == index\n",
    "    sub.set_title(str(classes[index]) + \":\" + str(res))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    del image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def predict_bird_from_file(fn, verbose=True):\n",
    "    image = Image.open(fn)\n",
    "    image = image.convert('RGB')\n",
    "    predicted_class_idx, confidence = predict_image(image)\n",
    "    predicted_class = class_name_list[predicted_class_idx]\n",
    "    if verbose:\n",
    "        display(image)\n",
    "        print('Class: {}, conf: {:.2f}'.format(predicted_class, confidence))\n",
    "    del image\n",
    "    return predicted_class_idx, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = IMAGES_DIR + '/' + test_df.iloc[8]['image_file_name']\n",
    "predict_bird_from_file(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = IMAGES_DIR + '/' + test_df.iloc[0]['image_file_name']\n",
    "predict_bird_from_file(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess prediction performance against validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.GnBu):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), \n",
    "                                  range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if cm[i, j] > thresh else 'black')\n",
    "    plt.tight_layout()\n",
    "    plt.gca().set_xticklabels(class_name_list)\n",
    "    plt.gca().set_yticklabels(class_name_list)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def create_and_plot_confusion_matrix(actual, predicted):\n",
    "    cnf_matrix = confusion_matrix(actual, np.asarray(predicted),labels=range(len(class_name_list)))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=range(len(class_name_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import Image, display\n",
    "\n",
    "# Iterate through entire dataframe, tracking predictions and accuracy. For mistakes, show the image, and the predicted and actual classes to help understand\n",
    "# where the model may need additional tuning.\n",
    "\n",
    "def test_image_df(df):\n",
    "    print('Testing {} images'.format(df.shape[0]))\n",
    "    num_errors = 0\n",
    "    preds = []\n",
    "    acts  = []\n",
    "    for i in range(df.shape[0]):\n",
    "        fname = df.iloc[i]['image_file_name']\n",
    "        act   = int(df.iloc[i]['class_id']) - 1\n",
    "        acts.append(act)\n",
    "        pred, conf = predict_bird_from_file(IMAGES_DIR + '/' + fname, verbose=False)\n",
    "        preds.append(pred)\n",
    "        if (pred != act):\n",
    "            num_errors += 1\n",
    "            print('ERROR on image index {} -- Pred: {} {:.2f}, Actual: {}'.format(i, \n",
    "                                                                    class_name_list[pred], conf, \n",
    "                                                                    class_name_list[act]))\n",
    "            display(Image.open(IMAGES_DIR + '/' + fname))\n",
    "    return num_errors, preds, acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = val_df.shape[0]\n",
    "num_errors, preds, acts = test_image_df(val_df)\n",
    "print('\\nAccuracy: {:.2f}, {}/{}'.format(1 - (num_errors/num_images), num_images - num_errors, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = test_df.shape[0]\n",
    "num_errors, preds, acts = test_image_df(test_df)\n",
    "print('\\nAccuracy: {:.2f}, {}/{}'.format(1 - (num_errors/num_images), num_images - num_errors, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model against previously unseen images\n",
    "Here we download images that the algorithm has not yet seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O northern-flicker-1.jpg https://upload.wikimedia.org/wikipedia/commons/5/5c/Northern_Flicker_%28Red-shafted%29.jpg\n",
    "!wget -q -O northern-cardinal-1.jpg https://cdn.pixabay.com/photo/2013/03/19/04/42/bird-94957_960_720.jpg\n",
    "!wget -q -O blue-jay-1.jpg https://cdn12.picryl.com/photo/2016/12/31/blue-jay-bird-feather-animals-b8ee04-1024.jpg\n",
    "!wget -q -O blue-jay-2.jpg https://www.pennington.com/-/media/Images/Pennington-NA/US/blog/Wild-Bird/Blue-Jays/Blue-Jay-Eating-Peanuts.jpg\n",
    "!wget -q -O hummingbird-1.jpg http://res.freestockphotos.biz/pictures/17/17875-hummingbird-close-up-pv.jpg\n",
    "!wget -q -O northern-cardinal-2.jpg https://www.allaboutbirds.org/guide/assets/photo/63667291-480px.jpg\n",
    "!wget -q -O american-goldfinch-1.jpg https://download.ams.birds.cornell.edu/api/v1/asset/59574291/medium\n",
    "!wget -q -O purple-finch-1.jpg https://indianaaudubon.org/wp-content/uploads/2016/04/PurpleFinchRyanSanderson-e1463792335814.jpg\n",
    "!wget -q -O purple-finch-2.jpg https://www.singing-wings-aviary.com/wp-content/uploads/2018/06/Purple-Finch.jpg\n",
    "!wget -q -O mallard-1.jpg https://www.herefordshirewt.org/sites/default/files/styles/node_hero_default/public/2018-01/Mallard%20%C2%A9%20Mark%20Hamblin.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_bird_from_file('american-goldfinch-1.jpg')\n",
    "predict_bird_from_file('northern-cardinal-1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
