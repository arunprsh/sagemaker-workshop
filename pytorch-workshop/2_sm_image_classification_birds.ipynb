{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch with Amazon SageMaker's training and hosting services\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Train the model](#Train-the-model)\n",
    "4. [Host the model](#Host-the-model)\n",
    "5. [Test the model](#Test-the-model)\n",
    "6. [Clean up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The previous lab performed training and prediction directly in the Jupyter notebook environment. With this lab, we transition to leveraging SageMaker's managed training and hosting services. To accomplish this, we use [Amazon SageMaker's PyTorch container](https://sagemaker.readthedocs.io/en/stable/using_pytorch.html), which lets you provide your training code as a Python script. The container also provides a flexible way for you to customize how inference inputs and outputs are handled over a REST interface. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before preparing the data, there are some initial steps required for setup. To train the image classification algorithm on Amazon SageMaker, we need to setup and authenticate the use of AWS services. To begin with, we need an AWS account role with SageMaker access. Here we will use the execution role the current notebook instance was given when it was created.  This role has necessary permissions, including access to your data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to identify the S3 bucket that you want to use for providing training and validation datasets.  It will  be used to store the tranied model artifacts as well. In this notebook, we use a default bucket for use with SageMaker in your account. Alternatively, you could use whatever bucket you would like.  We use an object prefix to help organize the bucket content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket() # or use your own custom bucket name\n",
    "prefix = 'DEMO-PYT-image-classification-birds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "This notebook assumes you have already downloaded and unpacked the dataset into your notebook instance as part of the first lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some parameters for the rest of the notebook to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a few parameters that help drive the rest of the notebook.  For example, `SAMPLE_ONLY` is defaulted to `True`. This will force the notebook to train on only a handful of species.  Setting `SAMPLE_ONLY` to false will make the notebook work with the entire dataset of 200 bird species.  This makes the training a more difficult challenge, and you will need to tune parameters and run more epochs.\n",
    "\n",
    "An `EXCLUDE_IMAGE_LIST` is defined as a mechanism to address any corrupt images from the dataset and ensure they do not disrupt the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "runtime = boto3.client(service_name='runtime.sagemaker')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# To speed up training and experimenting, you can use a small handful of species.\n",
    "# To see the full list of the classes available, look at the content of CLASSES_FILE.\n",
    "SAMPLE_ONLY  = True\n",
    "CLASSES = [13, 17, 35, 36]#, 47, 68, 73, 75, 87, 95, 120, 179, 192]\n",
    "#CLASSES = [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,  11,  12,  13,\n",
    "#           17, 35, 36, 47, 68, 73, 75, 87, 95, 120, 179, 192, 193]\n",
    "\n",
    "# Otherwise, you can use the full set of species\n",
    "if not SAMPLE_ONLY:\n",
    "    CLASSES = []\n",
    "    for c in range(200):\n",
    "        CLASSES += [c + 1]\n",
    "    prefix = prefix + '-full'\n",
    "\n",
    "BASE_DIR   = 'CUB_MINI/'\n",
    "BASE_DIR   = 'CUB_200_2011/'\n",
    "\n",
    "IMAGES_DIR = BASE_DIR + 'images/'\n",
    "CLASSES_FILE = BASE_DIR + 'classes.txt'\n",
    "\n",
    "EXCLUDE_IMAGE_LIST = ['087.Mallard/Mallard_0130_76836.jpg']\n",
    "\n",
    "SPLIT_RATIOS = (0.6, 0.2, 0.2)\n",
    "\n",
    "CLASS_COLS   = ['class_number','class_id']\n",
    "\n",
    "JOB_PREFIX   = 'pyt-ic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the list of bird species or dataset classes the model will be trained to predict. You will use this in the next notebook as well (3 - Batch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_df = pd.read_csv(CLASSES_FILE, sep=' ', names=CLASS_COLS, header=None)\n",
    "criteria = classes_df['class_number'].isin(CLASSES)\n",
    "classes_df = classes_df[criteria]\n",
    "\n",
    "class_name_list = sorted(classes_df['class_id'].unique().tolist())\n",
    "print(class_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train/val/test dataframes from our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we've moved the train/test split logic to a utilities script to keep the focus of the notebook on the actual training and hosting steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split\n",
    "train_df, val_df, test_df = split.get_train_val_dataframes(BASE_DIR, CLASSES, EXCLUDE_IMAGE_LIST, SPLIT_RATIOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data channels for Amazon SageMaker\n",
    "When using Amazon SageMaker's managed training service, you need to provide the datasets to the training algorithm. This is primarily handled via populating S3 buckets, and by indicating the location of data channels such as train, test, and validation. You also need to consider the data format. In our case, to keep things simple, we will populate the data channels with folders containing the original JPG images organized by class folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNEL_FOLDER = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate local data channels\n",
    "Here we populate the local channel folders, and we handle re-populating in case we have already run through this notebook with a different subset of bird species. The local content will be synchronized to our s3 bucket for access by the SageMaker training service. Note that if you are running this notebook with a large number of bird species, this step could take several minutes. You can skip these cells if you are re-running this notebook after already having populated these folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_for_channel(df, channel_name, verbose=False):\n",
    "    print('\\nCopying files for {} images in channel: {}...'.format(df.shape[0], channel_name))\n",
    "    for i in range(df.shape[0]):\n",
    "        target_fname = df.iloc[i]['image_file_name']\n",
    "        if verbose:\n",
    "            print(target_fname)\n",
    "        !cp $cwd/$IMAGES_DIR/$target_fname $cwd/$CHANNEL_FOLDER/$channel_name/$target_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(CHANNEL_FOLDER):\n",
    "    print('About to remove {}/{}'.format(cwd, CHANNEL_FOLDER))\n",
    "    perform_delete = input('Are you sure you want to remove it and start fresh (yes/no)?')\n",
    "    if perform_delete == 'yes':\n",
    "        for ch in ['train', 'test', 'validation']:\n",
    "            !rm -i -rf $CHANNEL_FOLDER/$ch/*\n",
    "            !rm -i -rf $CHANNEL_FOLDER/$ch\n",
    "            !mkdir  $CHANNEL_FOLDER/$ch\n",
    "\n",
    "        for c in class_name_list:\n",
    "            !mkdir $CHANNEL_FOLDER/train/$c\n",
    "            !mkdir $CHANNEL_FOLDER/validation/$c\n",
    "            !mkdir $CHANNEL_FOLDER/test/$c\n",
    "            \n",
    "        copy_files_for_channel(val_df,   'validation')\n",
    "        copy_files_for_channel(test_df,  'test')\n",
    "        copy_files_for_channel(train_df, 'train')\n",
    "else:\n",
    "    print('Channel folder does not yet exist. Creating it and subfolders for each channel.')\n",
    "    os.mkdir(CHANNEL_FOLDER)\n",
    "    os.mkdir(CHANNEL_FOLDER + '/validation')\n",
    "    os.mkdir(CHANNEL_FOLDER + '/test')\n",
    "    os.mkdir(CHANNEL_FOLDER + '/train')\n",
    "    for c in class_name_list:\n",
    "        os.mkdir('{}/{}/{}'.format(CHANNEL_FOLDER, 'validation', c))\n",
    "        os.mkdir('{}/{}/{}'.format(CHANNEL_FOLDER, 'test', c))\n",
    "        os.mkdir('{}/{}/{}'.format(CHANNEL_FOLDER, 'train', c))\n",
    "    \n",
    "    copy_files_for_channel(val_df,   'validation')\n",
    "    copy_files_for_channel(test_df,  'test')\n",
    "    copy_files_for_channel(train_df, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload content of each data channel to S3\n",
    "Note that for local mode training, this copy to s3 is not necessary. SageMaker local mode is helpful for early iterations of the development of a new model. Once the approach is more stable, you typically then use SageMaker training jobs on larger sets of data and with additional epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('About to uploaded your image data to s3://{}/{}'.format(bucket, prefix))\n",
    "print('This will ensure you have the latest data for training, but it takes a while if using the full 200 classes.')\n",
    "if input('Are you sure you want to replace your s3 images?') == 'yes':\n",
    "    print('Clearing out s3://{}/{}/'.format(bucket, prefix))\n",
    "    for ch in ['train', 'test', 'validation']:\n",
    "        !aws s3 rm --quiet --recursive s3://$bucket/$prefix/$ch/\n",
    "    print('\\nSynchronizing local data channels with s3...')\n",
    "    !aws s3 sync $CHANNEL_FOLDER s3://$bucket/$prefix/\n",
    "\n",
    "print('contents of s3://{}/{}/'.format(bucket, prefix))\n",
    "!aws s3 ls s3://$bucket/$prefix/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model \n",
    "When using SageMaker's TensorFlow container, the custom PyTorch training code is provided via a Python script in a separate file that gets passed to SageMaker. For our example, that script is shown below for completeness in the notebook. Study that code before proceeding to the actual training. Pay attention to any differences from the code you used in the first lab when training directly in the notebook:\n",
    "\n",
    "- Use of script parameters to make the code more flexible. We will take advantage of these parameters later on when doing automatic model tuning.\n",
    "- Use of PyTorch ImageFolder to provide datasets directly on top of the folders that SageMaker establishes for each input channel.\n",
    "- Different approach to model saving to be compatible with SageMaker's use of TensorFlow Serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pygmentize 'code/train-resnet.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train in local mode, it is necessary to have Docker Compose or NVIDIA-Docker-Compose (for GPU) installed in the notebook instance.  This simple setup script is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash ./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the SageMaker training job using the PyTorch container\n",
    "Here we establish the PyTorch estimator object. Note that this notebook code is designed to support SageMaker remote training jobs as well as local mode training. Set `local = False` to use SageMaker's training service on separate managed training instances. Metric definitions are provided so that you can visualize metrics from the SageMaker console as well as from CloudWatch. These same metrics can be used when optimizing your model with automatic model tuning.\n",
    "\n",
    "For this lab, run the whole notebook with `local = True` first. Once you have understood all that is going on, and have seen it work successfully, change to `local = False` and re-run the rest of the notebook.\n",
    "\n",
    "### A note about instance types and account limits\n",
    "You may find yourself running into an error like this:\n",
    "\n",
    "````\n",
    "ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.8xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.\n",
    "````\n",
    "To avoid customers getting unexpected bills for more powerful and more expensive instance usage, accounts are established by default with limited access to certain instance types. These are soft limits that can be raised by contacting AWS support. This lab defaults to a powerful GPU instance type, but you can run it on a lower-powered instance type. In such a case, you will pay less, but your training jobs will take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import logging\n",
    "\n",
    "FRAMEWORK_VERSION = '1.3.1'\n",
    "\n",
    "local = False\n",
    "if (local):\n",
    "    train_instance_type  = 'local'\n",
    "    train_instance_count = 1\n",
    "    serve_instance_type  = 'local'\n",
    "else:\n",
    "    train_instance_type  = 'ml.p3.2xlarge' \n",
    "    train_instance_count = 1\n",
    "    serve_instance_type  = 'ml.c5.4xlarge' #p3.2xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'initial_epochs': 25 if SAMPLE_ONLY else 100, \n",
    "                   'dropout' : 0.5,\n",
    "                   'data_dir': '/opt/ml/input/data',\n",
    "                   'backend' : 'gloo' # for cpu training, use: 'tcp' or 'gloo'; for gpu, use: 'gloo' or 'nccl'\n",
    "                  }\n",
    "\n",
    "metric_definitions=[{'Name' : 'validation:acc', \n",
    "                     'Regex': '.*Test accuracy: (.*$)'},\n",
    "                    {'Name' : 'validation:loss', \n",
    "                     'Regex': '.*Test loss: (.*).. Test ac.*'},\n",
    "                    {'Name' : 'train:loss', \n",
    "                     'Regex': '.*Train loss: (.*).. Test lo.*'}]\n",
    "\n",
    "estimator = PyTorch(entry_point='train-resnet.py',\n",
    "                       source_dir='code',\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=train_instance_count,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       framework_version=FRAMEWORK_VERSION, \n",
    "                       debugger_hook_config=False,  # https://tt.amazon.com/0305452782, https://answers.amazon.com/questions/93236\n",
    "                       py_version='py3',\n",
    "                       base_job_name=JOB_PREFIX)\n",
    "\n",
    "## To use Spot instances for SageMaker training, add this set of parameters to your\n",
    "## call above when creating the TensorFlow estimator object:\n",
    "##\n",
    "######    train_use_spot_instances=True, train_max_run=2*60*60, train_max_wait=3*60*60,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we establish pointers to where each data channel is located. For local mode training, these will be in the local file system. For remote training, the s3 data channel copies are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "print('Local data dir: {}'.format(data_dir))\n",
    "\n",
    "if (local):\n",
    "    train_in = f'file://{data_dir}/train'\n",
    "    test_in  = f'file://{data_dir}/test'\n",
    "    val_in   = f'file://{data_dir}/validation'\n",
    "else:\n",
    "    s3_base = 's3://{}/{}'.format(bucket, prefix)\n",
    "    train_in = f'{s3_base}/train'\n",
    "    test_in  = f'{s3_base}/test'\n",
    "    val_in   = f'{s3_base}/validation'\n",
    "\n",
    "inputs = {'train':train_in, 'test': test_in, 'validation': val_in}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tell the estimator to fit the model. For local mode, this training is performed in the current notebook instance. For remote training, new instances are launched and training is performed on those separate instances. No matter which technique you use, the training script can count on the same interface (script parameters, specific SageMaker environment variables, consistent location of data channels in `/opt/ml/input/data`, and saving to `/opt/ml/model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "print(f'Completed training job: {training_job_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the fit method is complete, we can simply call deploy (`estimator.deploy(...)`) to publish the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, \n",
    "                             instance_type=serve_instance_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using an endpoint that was already in service, you can establish a predictor object using the following code.\n",
    "#\n",
    "#from sagemaker import RealTimePredictor\n",
    "#predictor = RealTimePredictor(endpoint='tensorflow-inference-2020-02-14-12-36-15-896') #<put-endpoint-name-here>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using the SageMaker PyTorch endpoint\n",
    "When calling your image classifier hosted in a SageMaker endpoint, the call to the predictor is slightly different than what you used in the first lab:\n",
    "\n",
    "1. In lab 1, we preprocessed the image bytes, which is specific to our base ResNet model. Instead, here we pass the image bytes directly. This hides the implementation detail of a model-specific preprocessing of the image as part of the endpoint. This gives us flexibility to switch to a different implementation down the road (e.g., MobileNetV2, InceptionV3, ...).\n",
    "2. We indicate the content type for the REST call, in this case `application/x-image`.\n",
    "3. The results come back as a list of probabilities, one for each possible class.\n",
    "\n",
    "The rest of the code remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import io, time\n",
    "import numpy as np\n",
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "def predict_bird_from_file(fn, verbose=True):\n",
    "    pil_img = Image.open(fn)\n",
    "    buf = io.BytesIO()\n",
    "    pil_img.save(buf, format='JPEG')\n",
    "    byte_im = buf.getvalue()\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    predictor.content_type = 'application/x-image'\n",
    "    predictor.serializer   = None\n",
    "    predictor.deserializer = json_deserializer\n",
    "    \n",
    "    result = predictor.predict(byte_im)\n",
    "    \n",
    "    if verbose:\n",
    "        print('...took {:.2f} seconds'.format(time.time() - start_time))\n",
    "        predicted_class_idx = np.argmax(result)\n",
    "        predicted_class = class_name_list[predicted_class_idx]\n",
    "        confidence = result[predicted_class_idx]\n",
    "        display(Image.open(fn))\n",
    "        print(f'Class: {predicted_class}, confidence: {confidence:.2f}')\n",
    "    del pil_img\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_bird_from_file('northern-cardinal-1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.GnBu):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), \n",
    "                                  range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.gca().set_xticklabels(class_name_list)\n",
    "    plt.gca().set_yticklabels(class_name_list)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def create_and_plot_confusion_matrix(actual, predicted):\n",
    "    cnf_matrix = confusion_matrix(actual, np.asarray(predicted),labels=range(len(class_name_list)))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=range(len(class_name_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess prediction performance against validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through entire dataframe, tracking predictions and accuracy. For mistakes, show the image, and the predicted and actual classes to help understand\n",
    "# where the model may need additional tuning.\n",
    "\n",
    "def test_image_df(df, verbose=True):\n",
    "    print('Testing {} images'.format(df.shape[0]))\n",
    "    num_errors = 0\n",
    "    preds = []\n",
    "    acts  = []\n",
    "    for i in range(df.shape[0]):\n",
    "        fname = df.iloc[i]['image_file_name']\n",
    "        act   = int(df.iloc[i]['class_id']) - 1\n",
    "        acts.append(act)\n",
    "        prediction_list = predict_bird_from_file(IMAGES_DIR + '/' + fname, verbose=False)\n",
    "        pred = np.argmax(prediction_list)\n",
    "        conf = prediction_list[pred]\n",
    "        preds.append(pred)\n",
    "        if (pred != act):\n",
    "            num_errors += 1\n",
    "            print('ERROR on image index {} -- Pred: {} {:.2f}, Actual: {}'.format(i, \n",
    "                                                                   class_name_list[pred], conf, \n",
    "                                                                   class_name_list[act]))\n",
    "            if verbose:\n",
    "                print(f'    {prediction_list}')\n",
    "                display(Image.open(IMAGES_DIR + '/' + fname))\n",
    "    return num_errors, preds, acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def hit_endpoint_with_load(rough_num_predictions):\n",
    "    num_val_images = val_df.shape[0]\n",
    "    times_through_dataset = round(rough_num_predictions / num_val_images)\n",
    "    print(f'running through val dataset {times_through_dataset} times...\\n')\n",
    "    start_time = time.time()\n",
    "    for i in range(times_through_dataset):\n",
    "        num_errors, preds, acts = test_image_df(val_df, verbose=False)\n",
    "        print('\\nAccuracy: {:.2f}, {}/{}\\n'.format(1 - (num_errors/num_val_images), \n",
    "                                                   num_val_images - num_errors, num_val_images))\n",
    "    num_predictions = times_through_dataset * num_val_images\n",
    "    elapsed_wall_time = time.time() - start_time\n",
    "    print(f'\\nTook {elapsed_wall_time} seconds to predict on {num_predictions} images')\n",
    "    print('{:.1f} ms/image.\\n'.format(elapsed_wall_time / num_predictions * 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#hit_endpoint_with_load(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = val_df.shape[0]\n",
    "num_errors, preds, acts = test_image_df(val_df)\n",
    "print('\\nAccuracy: {:.2f}, {}/{}'.format(1 - (num_errors/num_images), num_images - num_errors, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "num_images = test_df.shape[0]\n",
    "num_errors, preds, acts = test_image_df(test_df)\n",
    "print('\\nAccuracy: {:.2f}, {}/{}'.format(1 - (num_errors/num_images), num_images - num_errors, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model against previously unseen images\n",
    "Here we download images that the algorithm has not yet seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_bird_from_file('northern-cardinal-2.jpg')\n",
    "predict_bird_from_file('northern-cardinal-1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "Finally, and importantly, to avoid being billed for an idle endpoint, here we delete the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
