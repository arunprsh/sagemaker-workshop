{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorFlow with Amazon SageMaker's training and hosting services\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup](#Setup)\n",
    "3. [Train the model](#Train-the-model)\n",
    "4. [Host the model](#Host-the-model)\n",
    "5. [Test the model](#Test-the-model)\n",
    "6. [Clean up](#Clean-up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The previous lab performed training and prediction directly in the Jupyter notebook environment. With this lab, we transition to leveraging SageMaker's managed training and hosting services. To accomplish this, we use [Amazon SageMaker's TensorFlow container](https://sagemaker.readthedocs.io/en/stable/using_tf.html), which lets you provide your training code as a Python script. The container also provides a flexible way for you to customize how inference inputs and outputs are handled over a REST interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before preparing the data, there are some initial steps required for setup. To train the image classification algorithm on Amazon SageMaker, we need to setup and authenticate the use of AWS services. To begin with, we need an AWS account role with SageMaker access. Here we will use the execution role the current notebook instance was given when it was created.  This role has necessary permissions, including access to your data in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to identify the S3 bucket that you want to use for providing training and validation datasets.  It will  be used to store the tranied model artifacts as well. In this notebook, we use a default bucket for use with SageMaker in your account. Alternatively, you could use whatever bucket you would like.  We use an object prefix to help organize the bucket content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = sess.default_bucket() # or use your own custom bucket name\n",
    "prefix = 'DEMO-TF-image-classification-birds'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "This notebook assumes you have already downloaded and unpacked the dataset into your notebook instance as part of the first lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set some parameters for the rest of the notebook to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a few parameters that help drive the rest of the notebook.  For example, `SAMPLE_ONLY` is defaulted to `True`. This will force the notebook to train on only a handful of species.  Setting `SAMPLE_ONLY` to false will make the notebook work with the entire dataset of 200 bird species.  This makes the training a more difficult challenge, and you will need to tune parameters and run more epochs.\n",
    "\n",
    "An `EXCLUDE_IMAGE_LIST` is defined as a mechanism to address any corrupt images from the dataset and ensure they do not disrupt the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "runtime = boto3.client(service_name='runtime.sagemaker')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# To speed up training and experimenting, you can use a small handful of species.\n",
    "# To see the full list of the classes available, look at the content of CLASSES_FILE.\n",
    "SAMPLE_ONLY  = True\n",
    "CLASSES = [13, 17] #, 35, 36, 47, 68, 73, 75, 87, 95, 120, 179, 192]\n",
    "\n",
    "# Otherwise, you can use the full set of species\n",
    "if not SAMPLE_ONLY:\n",
    "    CLASSES = []\n",
    "    for c in range(200):\n",
    "        CLASSES += [c + 1]\n",
    "    prefix = 'DEMO-TF-image-classification-birds-full'\n",
    "\n",
    "BASE_DIR   = 'CUB_200_2011/'\n",
    "IMAGES_DIR = BASE_DIR + 'images/'\n",
    "CLASSES_FILE = BASE_DIR + 'classes.txt'\n",
    "\n",
    "EXCLUDE_IMAGE_LIST = ['087.Mallard/Mallard_0130_76836.jpg']\n",
    "\n",
    "SPLIT_RATIOS = (0.6, 0.2, 0.2)\n",
    "\n",
    "CLASS_COLS      = ['class_number','class_id']\n",
    "\n",
    "JOB_PREFIX     = 'mpr-tf-ic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the list of bird species or dataset classes the model will be trained to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_df = pd.read_csv(CLASSES_FILE, sep=' ', names=CLASS_COLS, header=None)\n",
    "criteria = classes_df['class_number'].isin(CLASSES)\n",
    "classes_df = classes_df[criteria]\n",
    "\n",
    "class_name_list = sorted(classes_df['class_id'].unique().tolist())\n",
    "print(class_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train/val/test dataframes from our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we've moved the train/test split logic to a utilities script to keep the focus of the notebook on the actual training and hosting steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import split\n",
    "train_df, val_df, test_df = split.get_train_val_dataframes(BASE_DIR, CLASSES, EXCLUDE_IMAGE_LIST, SPLIT_RATIOS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the data channels for Amazon SageMaker\n",
    "When using Amazon SageMaker's managed training service, you need to provide the datasets to the training algorithm. This is primarily handled via populating S3 buckets, and by indicating the location of data channels such as train, test, and validation. You also need to consider the data format. In our case, to keep things simple, we will populate the data channels with folders containing the original JPG images organized by class folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNEL_FOLDER = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate local data channels\n",
    "Here we populate the local channel folders, and we handle re-populating in case we have already run through this notebook with a different subset of bird species. The local content will be synchronized to our s3 bucket for access by the SageMaker training service. Note that if you are running this notebook with a large number of bird species, this step could take several minutes. You can skip these cells if you are re-running this notebook after already having populated these folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_for_channel(df, channel_name, verbose=False):\n",
    "    print('\\nCopying files for {} images in channel: {}...'.format(df.shape[0], channel_name))\n",
    "    for i in range(df.shape[0]):\n",
    "        target_fname = df.iloc[i]['image_file_name']\n",
    "        if verbose:\n",
    "            print(target_fname)\n",
    "        !cp $cwd/$IMAGES_DIR/$target_fname $cwd/$CHANNEL_FOLDER/$channel_name/$target_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(CHANNEL_FOLDER):\n",
    "    print('About to remove {}/{}'.format(cwd, CHANNEL_FOLDER))\n",
    "    perform_delete = input('Are you sure you want to proceed (yes/no)?')\n",
    "    if perform_delete == 'yes':\n",
    "        for ch in ['train', 'test', 'validation']:\n",
    "            !rm -i -rf $CHANNEL_FOLDER/$ch/*\n",
    "            !rm -i -rf $CHANNEL_FOLDER/$ch\n",
    "            !mkdir  $CHANNEL_FOLDER/$ch\n",
    "\n",
    "        for c in class_name_list:\n",
    "            !mkdir $CHANNEL_FOLDER/train/$c\n",
    "            !mkdir $CHANNEL_FOLDER/validation/$c\n",
    "            !mkdir $CHANNEL_FOLDER/test/$c\n",
    "            \n",
    "        copy_files_for_channel(val_df,   'validation')\n",
    "        copy_files_for_channel(test_df,  'test')\n",
    "        copy_files_for_channel(train_df, 'train')\n",
    "else:\n",
    "    print('Channel folder does not yet exist. Creating it and subfolders for each channel.')\n",
    "    os.mkdir(CHANNEL_FOLDER)\n",
    "    os.mkdir(CHANNEL_FOLDER + '/validation')\n",
    "    os.mkdir(CHANNEL_FOLDER + '/test')\n",
    "    os.mkdir(CHANNEL_FOLDER + '/train')\n",
    "    for c in class_name_list:\n",
    "        os.mkdir('{}/{}/{}'.format(CHANNEL_FOLDER, 'validation', c))\n",
    "        os.mkdir('{}/{}/{}'.format(CHANNEL_FOLDER, 'test', c))\n",
    "        os.mkdir('{}/{}/{}'.format(CHANNEL_FOLDER, 'train', c))\n",
    "    \n",
    "    copy_files_for_channel(val_df,   'validation')\n",
    "    copy_files_for_channel(test_df,  'test')\n",
    "    copy_files_for_channel(train_df, 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload content of each data channel to S3\n",
    "Note that for local mode training, this copy to s3 is not necessary. SageMaker local mode is helpful for early iterations of the development of a new model. Once the approach is more stable, you typically then use SageMaker training jobs on larger sets of data and with additional epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if input('Are you sure you want to replace your s3 images?') == 'yes':\n",
    "    print('Clearing out s3://{}/{}/'.format(bucket, prefix))\n",
    "    for ch in ['train', 'test', 'validation']:\n",
    "        !aws s3 rm --quiet --recursive s3://$bucket/$prefix/$ch/\n",
    "    print('\\nSynchronizing local data channels with s3...')\n",
    "    !aws s3 sync $CHANNEL_FOLDER s3://$bucket/$prefix/\n",
    "\n",
    "print('contents of s3://{}/{}/'.format(bucket, prefix))\n",
    "!aws s3 ls s3://$bucket/$prefix/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model \n",
    "When using SageMaker's TensorFlow container, the custom TensorFlow training code is provided via a Python script in a separate file that gets passed to SageMaker. For our example, that script is shown below for completeness in the notebook. Study that code before proceeding to the actual training. Pay attention to any differences from the code you used in the first lab when training directly in the notebook:\n",
    "\n",
    "- Use of script parameters to make the code more flexible. We will take advantage of these parameters later on when doing automatic model tuning.\n",
    "- Copying of `inference.py` and `requirements.txt` to the code directory for use with TensorFlow Serving at inference time.\n",
    "- Different approach to model saving to be compatible with SageMaker's use of TensorFlow Serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;49;00m\r\n",
      "\u001b[37m# may not use this file except in compliance with the License. A copy of\u001b[39;49;00m\r\n",
      "\u001b[37m# the License is located at\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m#     http://aws.amazon.com/apache2.0/\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# or in the \"license\" file accompanying this file. This file is\u001b[39;49;00m\r\n",
      "\u001b[37m# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\u001b[39;49;00m\r\n",
      "\u001b[37m# ANY KIND, either express or implied. See the License for the specific\u001b[39;49;00m\r\n",
      "\u001b[37m# language governing permissions and limitations under the License.\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.applications.mobilenet_v2\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MobileNetV2, preprocess_input\r\n",
      "LAST_FROZEN_LAYER = \u001b[34m20\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.preprocessing.image\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ImageDataGenerator\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.layers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.models\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Sequential, Model\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.optimizers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SGD, Adam, RMSprop\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.callbacks\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m ModelCheckpoint\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m backend \u001b[34mas\u001b[39;49;00m K\r\n",
      "\r\n",
      "\u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTF version: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tf.__version__))\r\n",
      "\u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mKeras version: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorflow.keras.__version__))\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m argmax\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\r\n",
      "\r\n",
      "HEIGHT = \u001b[34m224\u001b[39;49;00m\r\n",
      "WIDTH  = \u001b[34m224\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[37m# Need to copy these files to the code directory, else the SageMaker endpoint will not use them.\u001b[39;49;00m\r\n",
      "\u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mCopying inference source files...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m os.path.exists(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/model/code\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\r\n",
      "    os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mmkdir /opt/ml/model/code\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mcp inference.py /opt/ml/model/code\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "os.system(\u001b[33m'\u001b[39;49;00m\u001b[33mcp requirements.txt /opt/ml/model/code\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mFiles after copy:\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\u001b[34mprint\u001b[39;49;00m(glob.glob(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/model/code/*\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mbuild_finetune_model\u001b[39;49;00m(base_model, dropout, fc_layers, num_classes):\r\n",
      "    \u001b[37m# Freeze all base layers\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m base_model.layers:\r\n",
      "        layer.trainable = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "\r\n",
      "    x = base_model.output\r\n",
      "    x = Flatten()(x)\r\n",
      "    \u001b[34mfor\u001b[39;49;00m fc \u001b[35min\u001b[39;49;00m fc_layers:\r\n",
      "        x = Dense(fc, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(x) \r\n",
      "        \u001b[34mif\u001b[39;49;00m (dropout != \u001b[34m0.0\u001b[39;49;00m):\r\n",
      "            x = Dropout(dropout)(x)\r\n",
      "\r\n",
      "    \u001b[37m# New softmax layer\u001b[39;49;00m\r\n",
      "    predictions = Dense(num_classes, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, name=\u001b[33m'\u001b[39;49;00m\u001b[33moutput\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(x) \r\n",
      "    \r\n",
      "    finetune_model = Model(inputs=base_model.input, outputs=predictions)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m finetune_model\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_data_generators\u001b[39;49;00m():\r\n",
      "    train_datagen =  ImageDataGenerator(\r\n",
      "              preprocessing_function=preprocess_input,\r\n",
      "              rotation_range=\u001b[34m70\u001b[39;49;00m,\r\n",
      "              brightness_range=(\u001b[34m0.7\u001b[39;49;00m, \u001b[34m1.0\u001b[39;49;00m),\r\n",
      "              width_shift_range=\u001b[34m0.2\u001b[39;49;00m,\r\n",
      "              height_shift_range=\u001b[34m0.2\u001b[39;49;00m,\r\n",
      "              shear_range=\u001b[34m0.2\u001b[39;49;00m,\r\n",
      "              zoom_range=\u001b[34m0.2\u001b[39;49;00m,\r\n",
      "              horizontal_flip=\u001b[36mTrue\u001b[39;49;00m,\r\n",
      "              vertical_flip=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    val_datagen  = ImageDataGenerator(preprocessing_function=preprocess_input)\r\n",
      "    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\r\n",
      "\r\n",
      "    train_gen = train_datagen.flow_from_directory(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                                  target_size=(HEIGHT, WIDTH), \r\n",
      "                                                  batch_size=args.batch_size)\r\n",
      "    test_gen = train_datagen.flow_from_directory(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                                  target_size=(HEIGHT, WIDTH), \r\n",
      "                                                  batch_size=args.batch_size)\r\n",
      "    val_gen = train_datagen.flow_from_directory(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                                  target_size=(HEIGHT, WIDTH), \r\n",
      "                                                  batch_size=args.batch_size)\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m train_gen, test_gen, val_gen\r\n",
      "    \r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m(args):\r\n",
      "    \u001b[37m# Create data generators for feeding training and evaluation based on data provided to us\u001b[39;49;00m\r\n",
      "    \u001b[37m# by the SageMaker TensorFlow container\u001b[39;49;00m\r\n",
      "    train_gen, test_gen, val_gen = create_data_generators()\r\n",
      "\r\n",
      "    base_model = MobileNetV2(weights=\u001b[33m'\u001b[39;49;00m\u001b[33mimagenet\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                          include_top=\u001b[36mFalse\u001b[39;49;00m, \r\n",
      "                          input_shape=(HEIGHT, WIDTH, \u001b[34m3\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[37m# Here we extend the base model with additional fully connected layers, dropout for avoiding\u001b[39;49;00m\r\n",
      "    \u001b[37m# overfitting to the training dataset, and a classification layer\u001b[39;49;00m\r\n",
      "    fully_connected_layers = []\r\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(args.num_fully_connected_layers):\r\n",
      "        fully_connected_layers.append(\u001b[34m1024\u001b[39;49;00m)\r\n",
      "\r\n",
      "    num_classes = \u001b[36mlen\u001b[39;49;00m(glob.glob(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/input/data/train/*\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    model = build_finetune_model(base_model, \r\n",
      "                                  dropout=args.dropout, \r\n",
      "                                  fc_layers=fully_connected_layers, \r\n",
      "                                  num_classes=num_classes)\r\n",
      "\r\n",
      "    opt = RMSprop(lr=args.initial_lr)\r\n",
      "    model.compile(opt, loss=\u001b[33m'\u001b[39;49;00m\u001b[33mcategorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mBeginning training...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    NUM_EPOCHS  = args.fine_tuning_epochs\r\n",
      "    FINE_TUNING = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "    \r\n",
      "    num_train_images = \u001b[36mlen\u001b[39;49;00m(train_gen.filepaths)\r\n",
      "    num_val_images   = \u001b[36mlen\u001b[39;49;00m(val_gen.filepaths)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m FINE_TUNING:\r\n",
      "        history = model.fit_generator(train_gen, epochs=NUM_EPOCHS, workers=\u001b[34m8\u001b[39;49;00m, \r\n",
      "                               steps_per_epoch=num_train_images // args.batch_size, \r\n",
      "                               validation_data=val_gen, validation_steps=num_val_images // args.batch_size,\r\n",
      "                               shuffle=\u001b[36mTrue\u001b[39;49;00m) \r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[37m# Train for a few epochs\u001b[39;49;00m\r\n",
      "        model.fit_generator(train_gen, epochs=args.initial_epochs, workers=\u001b[34m8\u001b[39;49;00m, \r\n",
      "                               steps_per_epoch=num_train_images // args.batch_size, \r\n",
      "                               validation_data=val_gen, validation_steps=num_val_images // args.batch_size,\r\n",
      "                               shuffle=\u001b[36mTrue\u001b[39;49;00m) \r\n",
      "\r\n",
      "        \u001b[37m# Now fine tune the last set of layers in the model\u001b[39;49;00m\r\n",
      "        \u001b[34mfor\u001b[39;49;00m layer \u001b[35min\u001b[39;49;00m model.layers[LAST_FROZEN_LAYER:]:\r\n",
      "            layer.trainable = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "\r\n",
      "        fine_tuning_lr = args.fine_tuning_lr\r\n",
      "        model.compile(optimizer=SGD(lr=fine_tuning_lr, momentum=\u001b[34m0.9\u001b[39;49;00m, decay=fine_tuning_lr / NUM_EPOCHS), \r\n",
      "                      loss=\u001b[33m'\u001b[39;49;00m\u001b[33mcategorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "        history = model.fit_generator(train_gen, epochs=NUM_EPOCHS, workers=\u001b[34m8\u001b[39;49;00m, \r\n",
      "                               steps_per_epoch=num_train_images // args.batch_size, \r\n",
      "                               validation_data=val_gen, validation_steps=num_val_images // args.batch_size,\r\n",
      "                               shuffle=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has been fit.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving model to /opt/ml/model...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[37m# Note that this method of saving does produce a warning about not containing the train and evaluate graphs.\u001b[39;49;00m\r\n",
      "    \u001b[37m# The resulting saved model works fine for inference. It will simply not support incremental training. If that\u001b[39;49;00m\r\n",
      "    \u001b[37m# is needed, one can use model checkpoints and save those.\u001b[39;49;00m\r\n",
      "    tf.contrib.saved_model.save_keras_model(model, \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m...DONE saving model!\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mExiting training script.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m==\u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--initial_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m5\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--fine_tuning_epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m30\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m16\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--initial_lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.0001\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--fine_tuning_lr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.00001\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--dropout\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.5\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_fully_connected_layers\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1\u001b[39;49;00m)\r\n",
      "    \u001b[37m# input data and model directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33margs: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args))\r\n",
      "    \r\n",
      "    main(args)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'code/train-mobilenet.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;49;00m\r\n",
      "\u001b[37m# may not use this file except in compliance with the License. A copy of\u001b[39;49;00m\r\n",
      "\u001b[37m# the License is located at\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m#     http://aws.amazon.com/apache2.0/\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# or in the \"license\" file accompanying this file. This file is\u001b[39;49;00m\r\n",
      "\u001b[37m# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\u001b[39;49;00m\r\n",
      "\u001b[37m# ANY KIND, either express or implied. See the License for the specific\u001b[39;49;00m\r\n",
      "\u001b[37m# language governing permissions and limitations under the License.\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m namedtuple\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mPIL\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Image\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.preprocessing\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m image\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras.applications.mobilenet_v2\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MobileNetV2, preprocess_input\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m argmax\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\r\n",
      "logging.basicConfig(level=logging.DEBUG)\r\n",
      "logging.getLogger().setLevel(logging.DEBUG)\r\n",
      "tf.logging.set_verbosity(tf.logging.ERROR)\r\n",
      "\r\n",
      "HEIGHT = \u001b[34m224\u001b[39;49;00m\r\n",
      "WIDTH  = \u001b[34m224\u001b[39;49;00m\r\n",
      "\r\n",
      "Context = namedtuple(\u001b[33m'\u001b[39;49;00m\u001b[33mContext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                     \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_name, model_version, method, rest_uri, grpc_uri, \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "                     \u001b[33m'\u001b[39;49;00m\u001b[33mcustom_attributes, request_content_type, accept_header\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_handler\u001b[39;49;00m(data, context):\r\n",
      "\r\n",
      "    logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33min input_handler\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mif\u001b[39;49;00m context.request_content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-image\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "\r\n",
      "        stream = io.BytesIO(data.read())\r\n",
      "        img = Image.open(stream)\r\n",
      "        img = img.resize((WIDTH, HEIGHT))\r\n",
      "        x = np.asarray(img)\r\n",
      "        curr_shape = x.shape\r\n",
      "        new_shape = (\u001b[34m1\u001b[39;49;00m,) + curr_shape\r\n",
      "        x = x.reshape(new_shape)\r\n",
      "        instance = preprocess_input(x)\r\n",
      "        final_input = instance.tolist()\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m json.dumps({\u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: instance.tolist()})\r\n",
      "\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        _return_error(\u001b[34m415\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mUnsupported content type \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(context.request_content_type \u001b[35mor\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mUnknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m data\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_handler\u001b[39;49;00m(data, context):\r\n",
      "    \u001b[33m\"\"\"Post-process TensorFlow Serving output before it is returned to the client.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m        data (obj): the TensorFlow serving response\u001b[39;49;00m\r\n",
      "\u001b[33m        context (Context): an object containing request and configuration details\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m        (bytes, string): data to return to client, response content type\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m data.status_code != \u001b[34m200\u001b[39;49;00m:\r\n",
      "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(data.content.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    response_content_type = context.accept_header\r\n",
      "    prediction = data.content\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m prediction, response_content_type\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_return_error\u001b[39;49;00m(code, message):\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mError: {}, {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mstr\u001b[39;49;00m(code), message))\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pillow\r\n",
      "numpy"
     ]
    }
   ],
   "source": [
    "!cat ./code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using local mode, this simple setup script is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash ./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the SageMaker training job using the TensorFlow container\n",
    "Here we establish the Tensorflow estimator object. Note that this notebook code is designed to support SageMaker remote training jobs as well as local mode training. Set `local = False` to use SageMaker's training service on separate managed training instances. Metric definitions are provided so that you can visualize metrics from the SageMaker console as well as from CloudWatch. These same metrics can be used when optimizing your model with automatic model tuning.\n",
    "\n",
    "For this lab, run the whole notebook with `local = True` first. Once you have understood all that is going on, and have seen it work successfully, change to `local = False` and re-run the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "import logging\n",
    "\n",
    "TF_FRAMEWORK_VERSION = '1.12'\n",
    "\n",
    "local = False\n",
    "if (local):\n",
    "    train_instance_type = 'local'\n",
    "    serve_instance_type = 'local'\n",
    "else:\n",
    "    train_instance_type = 'ml.p3.8xlarge' \n",
    "    serve_instance_type = 'ml.c5.xlarge'\n",
    "\n",
    "hyperparameters = {'fine_tuning_epochs': 25, \n",
    "                   'dropout': 0.5,\n",
    "                   'data_dir': '/opt/ml/input/data'}\n",
    "\n",
    "metric_definitions=[{'Name' : 'validation:acc', \n",
    "                     'Regex': '.*step.* - val_acc: (.*$)'},\n",
    "                    {'Name' : 'validation:loss', \n",
    "                     'Regex': '- val_loss: (.*?) '},\n",
    "                   {'Name' : 'acc', \n",
    "                     'Regex': '.*step.* - acc: (.*?) '},\n",
    "                    {'Name' : 'loss', \n",
    "                     'Regex': '.*step.* - loss: (.*?) '}]\n",
    "\n",
    "estimator = TensorFlow(entry_point='train-mobilenet.py',\n",
    "                       source_dir='code',\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       metric_definitions=metric_definitions,\n",
    "                       role=sagemaker.get_execution_role(),\n",
    "                       framework_version=TF_FRAMEWORK_VERSION, \n",
    "                       py_version='py3',\n",
    "                       base_job_name=JOB_PREFIX,\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we establish pointers to where each data channel is located. For local mode training, these will be in the local file system. For remote training, the s3 data channel copies are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "print('Local data dir: {}'.format(data_dir))\n",
    "\n",
    "if (local):\n",
    "    train_in = f'file://{data_dir}/train'\n",
    "    test_in  = f'file://{data_dir}/test'\n",
    "    val_in   = f'file://{data_dir}/validation'\n",
    "else:\n",
    "    s3_base = 's3://{}/{}'.format(bucket, prefix)\n",
    "    train_in = f'{s3_base}/train'\n",
    "    test_in  = f'{s3_base}/test'\n",
    "    val_in   = f'{s3_base}/validation'\n",
    "\n",
    "inputs = {'train':train_in, 'test': test_in, 'validation': val_in}\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we tell the estimator to fit the model. For local mode, this training is performed in the current notebook instance. For remote training, new instances are launched and training is performed on those separate instances. No matter which technique you use, the training script can count on the same interface (script parameters, specific SageMaker environment variables, consistent location of data channels in `/opt/ml/input/data`, and saving to `/opt/ml/model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(inputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Completed training job: {}'.format(estimator.latest_training_job.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Host the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simple case, once the fit method is complete, we can simply call deploy to publish the trained model. However, if you are iterating on various implementations of `inference.py`, you can avoid having to retrain the model by simply replacing the file inside of the model artifacts compressed zip file. This next cell runs a script that downloads the model artifacts, replaces the inference files, and uploads the new model artifacts zip for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local:\n",
    "    # re-deploy model with new inference.py without having to re-do training job\n",
    "    from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "    training_job_name = estimator.latest_training_job.name\n",
    "    model_artifacts = 's3://{}/{}/output/model.tar.gz'.format(bucket, training_job_name)\n",
    "    print(model_artifacts)\n",
    "\n",
    "    !bash ./replace-inference.sh $model_artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we deploy the updated model artifacts. If iterating on `inference.py` changes, this code re-creates the SageMaker model object and deploys it. Otherwise, simply deploy the model directly from the original estimator. You can use this approach once your `inference.py` code is stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local:\n",
    "    model = Model(model_data=model_artifacts, \n",
    "                  role=sagemaker.get_execution_role(),\n",
    "                  framework_version=TF_FRAMEWORK_VERSION,\n",
    "                  sagemaker_session=sess)\n",
    "\n",
    "    predictor = model.deploy(initial_instance_count=1, \n",
    "                         instance_type=serve_instance_type)\n",
    "else:\n",
    "    predictor = estimator.deploy(initial_instance_count=1, \n",
    "                               instance_type=serve_instance_type,\n",
    "                               endpoint_type='tensorflow-serving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using an endpoint that was already in service, you can establish a predictor object with the following code.\n",
    "#from sagemaker import RealTimePredictor\n",
    "#predictor = RealTimePredictor(endpoint='<put-endpoint-name-here>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from numpy import argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting using the SageMaker TensorFlow endpoint\n",
    "When calling your image classifier hosted in a SageMaker endpoint, the call to the predictor is slightly different than what you used in the first lab:\n",
    "\n",
    "1. In lab 1, we preprocessed the image bytes, which is specific to our base MobileNet model. Instead, here we pass the image bytes directly. This hides the implementation detail of a model-specific preprocessing of the image as part of the endpoint. This gives us flexibility to switch to a different implementation down the road (perhaps ResNet50).\n",
    "2. We indicate the content type for the rest call, in this case `application/x-image`.\n",
    "3. The results come back as a json document with a `predictions` object.\n",
    "\n",
    "The rest of the code remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "def predict_bird_from_file(fn, verbose=True):\n",
    "    with open(fn, 'rb') as img:\n",
    "        f = img.read()\n",
    "    x = bytearray(f)\n",
    "    \n",
    "    predictor.content_type = 'application/x-image'\n",
    "    predictor.serializer   = None\n",
    "    \n",
    "    results = predictor.predict(x)['predictions']\n",
    "\n",
    "    predicted_class_idx = argmax(results)\n",
    "    predicted_class = class_name_list[predicted_class_idx]\n",
    "    confidence = results[0][predicted_class_idx]\n",
    "    if verbose:\n",
    "        display(Image(fn))\n",
    "        print('Class: {}, confidence: {:.2f}'.format(predicted_class, confidence))\n",
    "    del img, x\n",
    "    return predicted_class_idx, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_bird_from_file('northern-cardinal-1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.GnBu):\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.grid(False)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), \n",
    "                                  range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.gca().set_xticklabels(class_name_list)\n",
    "    plt.gca().set_yticklabels(class_name_list)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def create_and_plot_confusion_matrix(actual, predicted):\n",
    "    cnf_matrix = confusion_matrix(actual, np.asarray(predicted),labels=range(len(class_name_list)))\n",
    "    plot_confusion_matrix(cnf_matrix, classes=range(len(class_name_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess prediction performance against validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "# Iterate through entire dataframe, tracking predictions and accuracy. For mistakes, show the image, and the predicted and actual classes to help understand\n",
    "# where the model may need additional tuning.\n",
    "\n",
    "def test_image_df(df):\n",
    "    print('Testing {} images'.format(df.shape[0]))\n",
    "    num_errors = 0\n",
    "    preds = []\n",
    "    acts  = []\n",
    "    for i in range(df.shape[0]):\n",
    "        fname = df.iloc[i]['image_file_name']\n",
    "        act   = int(df.iloc[i]['class_id']) - 1\n",
    "        acts.append(act)\n",
    "        pred, conf = predict_bird_from_file(IMAGES_DIR + '/' + fname, verbose=False)\n",
    "        preds.append(pred)\n",
    "        if (pred != act):\n",
    "            num_errors += 1\n",
    "            print('ERROR on image index {} -- Pred: {} {:.2f}, Actual: {}'.format(i, \n",
    "                                                                   class_name_list[pred], conf, \n",
    "                                                                   class_name_list[act]))\n",
    "            display(Image(filename=IMAGES_DIR + '/' + fname))\n",
    "    return num_errors, preds, acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = val_df.shape[0]\n",
    "num_errors, preds, acts = test_image_df(val_df)\n",
    "print('\\nAccuracy: {:.2f}, {}/{}'.format(1 - (num_errors/num_images), num_images - num_errors, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = test_df.shape[0]\n",
    "num_errors, preds, acts = test_image_df(test_df)\n",
    "print('\\nAccuracy: {:.2f}, {}/{}'.format(1 - (num_errors/num_images), num_images - num_errors, num_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model against previously unseen images\n",
    "Here we download images that the algorithm has not yet seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_bird_from_file('northern-cardinal-2.jpg')\n",
    "predict_bird_from_file('northern-cardinal-1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "Finally, and importantly, to avoid being billed for an idle endpoint, here we delete the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
